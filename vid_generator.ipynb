{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Generation using fine-tuned StyleGAN3 model\n",
    "\n",
    "This notebook contains code adapted from Class 7, titled *02_StyleGAN_inference.ipynb*, with help from Copilot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Setup\n",
    "\n",
    "First lets clone StyleGAN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And install additional libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've already pretrained two models, one for the title sections, and one for the rest of the film. I'll link to them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[main one]()\n",
    "\n",
    "[titles one]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add the StyleGAN repository to path temporarily in order to import some functions from torch_utils.\n",
    "\n",
    "I had to use my full path and insert it at the beginning of my path to make it work. (thanks to Copilot)\n",
    "\n",
    "```base_dir = r'C:\\Users\\iplow\\Documents\\code\\ExploringMachineIntelligence_Spring2024\\class-7\\stylegan3'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "base_dir = \"./stylegan3\"\n",
    "if base_dir not in sys.path:\n",
    "    sys.path.insert(0, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch_utils import misc\n",
    "from torch_utils.ops import upfirdn2d\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import legacy\n",
    "import dnnlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 2.3.1+cu121\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f'torch version {torch.__version__}')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_pkl_main = r'C:\\Users\\iplow\\Documents\\code\\coding-3-submission\\conor-output\\00007-stylegan3-t--gpus1-batch32-gamma32\\network-snapshot-000020.pkl'\n",
    "network_pkl_titles = r'C:\\Users\\iplow\\Documents\\code\\coding-3-submission\\conor-output-titles\\00003-stylegan3-t--gpus1-batch32-gamma32\\network-snapshot-000080.pkl'\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl_main) as f:\n",
    "    model = legacy.load_network_pkl(f)\n",
    "    g_model_1 = model['G'].eval().requires_grad_(False).to(device)\n",
    "\n",
    "with dnnlib.util.open_url(network_pkl_titles) as f:\n",
    "    model = legacy.load_network_pkl(f)\n",
    "    g_model_2 = model['G'].eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Projecting into latent space and generating frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple more imports needed for the projection and to interpolate between projected frames. \n",
    "\n",
    "Unfortunately there's not enough time to project into the latent space for every frame of the video, so we have to fill in the blanks with interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylegan3.dnnlib.util import open_url\n",
    "from utils import image_path_to_tensor\n",
    "from utils import run_projector\n",
    "\n",
    "from utils import slerp\n",
    "from base64 import b64encode\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# these are my added functions as I have a few more requirements\n",
    "from utils import image_directory_to_tensors\n",
    "from utils import get_ws_emas_for_scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch a feature extractor\n",
    "\n",
    "In order to move closer to the target style vector, we'll be using a pre-trained feature extractor to tell us how close we are. We'll use [VGG16](https://www.geeksforgeeks.org/vgg-16-cnn-model/) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "with open_url(url) as f:\n",
    "    vgg16 = torch.jit.load(f).eval().to(device)\n",
    "print('Using device:', device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the images for interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put all my video files, separated by cuts, into a folder. Then I got Copilot to generate a script which split each image into frames, at 1frame per second.\n",
    "\n",
    "This script is in the repository: [extract_frames.py](./extract_frames.py)\n",
    " \n",
    "The script taker this structure of files:\n",
    "\n",
    "![A picture of file structure containing a base directory titled \"original videos\" and two subdirectories titled respective to the AI model we want to use for those images.](journal_img\\arranged_videos.png)\n",
    "\n",
    "Where the subdirectories are named after the key for the model we want to use for those images.\n",
    "\n",
    "Now we're going to need the path to all of those images in code, and to create tensors from them. We're keeping the file structure from the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extracted_frames3/TITLES\\\\frame00000.png', 'extracted_frames3/TITLES\\\\frame00001.png', 'extracted_frames3/TITLES\\\\frame00002.png']\n"
     ]
    }
   ],
   "source": [
    "target_images_base_path = r\"extracted_frames/\"\n",
    "\n",
    "# Initialize an empty list to store lists of tensors from each subdirectory\n",
    "all_subdir_tensors_and_models = image_directory_to_tensors(target_images_base_path, g_model_1, g_model_2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "We're going to make a directory which will contain all of our generated frames for each cut. This way we can save progress after each generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory to save the model checkpoints\n",
    "if not os.path.exists('generated_frames'):\n",
    "    os.makedirs('generated_frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into the $w$ space\n",
    "\n",
    "We are now going to take our image, and project it into the $w$ space of StyleGAN. This process will start with a random vector, and make changes to the latent vector and noise input, until it converges on on the closest matching image in StyleGAN space to our input image. This is quite a long process, however if you want to shorten it you can change the the `step` variable to a smaller number if you want to reduce the amount of steps taken to find the closest match.\n",
    "\n",
    "I have modified the code to work with our file structure/ setup per cut in the film.\n",
    "\n",
    "This takes a long time and is really the main part of the project. IT took me 8 hours or so! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0/500 | loss: 0.4338529109954834\n",
      "image 10/500 | loss: 0.1981426179409027\n",
      "image 20/500 | loss: 0.19325152039527893\n",
      "image 30/500 | loss: 0.17646589875221252\n",
      "image 40/500 | loss: 0.16870340704917908\n",
      "image 50/500 | loss: 0.17523540556430817\n",
      "image 60/500 | loss: 0.17212903499603271\n",
      "image 70/500 | loss: 0.16980378329753876\n",
      "image 80/500 | loss: 0.18504494428634644\n",
      "image 90/500 | loss: 0.17219758033752441\n",
      "image 100/500 | loss: 0.17117904126644135\n",
      "image 110/500 | loss: 0.17082010209560394\n",
      "image 120/500 | loss: 0.1705816239118576\n",
      "image 130/500 | loss: 0.17958956956863403\n",
      "image 140/500 | loss: 0.1705007702112198\n",
      "image 150/500 | loss: 0.16997091472148895\n",
      "image 160/500 | loss: 0.16975969076156616\n",
      "image 170/500 | loss: 0.1699252426624298\n",
      "image 180/500 | loss: 0.16972672939300537\n",
      "image 190/500 | loss: 0.20126058161258698\n",
      "image 200/500 | loss: 0.1720017045736313\n",
      "image 210/500 | loss: 0.17230188846588135\n",
      "image 220/500 | loss: 0.1694135218858719\n",
      "image 230/500 | loss: 0.1692873239517212\n",
      "image 240/500 | loss: 0.1692652851343155\n",
      "image 250/500 | loss: 0.17716139554977417\n",
      "image 260/500 | loss: 0.21353617310523987\n",
      "image 270/500 | loss: 0.1692200005054474\n",
      "image 280/500 | loss: 0.16916143894195557\n",
      "image 290/500 | loss: 0.16934862732887268\n",
      "image 300/500 | loss: 0.1689753383398056\n",
      "image 310/500 | loss: 0.17364373803138733\n",
      "image 320/500 | loss: 0.1696576476097107\n",
      "image 330/500 | loss: 0.17220152914524078\n",
      "image 340/500 | loss: 0.1720140427350998\n",
      "image 350/500 | loss: 0.17096057534217834\n",
      "image 360/500 | loss: 0.17177066206932068\n",
      "image 370/500 | loss: 0.17932197451591492\n",
      "image 380/500 | loss: 0.20166809856891632\n",
      "image 390/500 | loss: 0.17482900619506836\n",
      "image 400/500 | loss: 0.16986899077892303\n",
      "image 410/500 | loss: 0.16908755898475647\n",
      "image 420/500 | loss: 0.1687292605638504\n",
      "image 430/500 | loss: 0.1680714190006256\n",
      "image 440/500 | loss: 0.16767838597297668\n",
      "image 450/500 | loss: 0.16908685863018036\n",
      "image 460/500 | loss: 0.1677035093307495\n",
      "image 470/500 | loss: 0.17126157879829407\n",
      "image 480/500 | loss: 0.18616200983524323\n",
      "image 490/500 | loss: 0.16720663011074066\n",
      "Scene 0/1, Frame 1/3 complete\n",
      "image 0/500 | loss: 0.41213977336883545\n",
      "image 10/500 | loss: 0.37585458159446716\n",
      "image 20/500 | loss: 0.3383276164531708\n",
      "image 30/500 | loss: 0.3318885266780853\n",
      "image 40/500 | loss: 0.30010986328125\n",
      "image 50/500 | loss: 0.310761421918869\n",
      "image 60/500 | loss: 0.32718148827552795\n",
      "image 70/500 | loss: 0.27916690707206726\n",
      "image 80/500 | loss: 0.28076374530792236\n",
      "image 90/500 | loss: 0.2728954553604126\n",
      "image 100/500 | loss: 0.27804964780807495\n",
      "image 110/500 | loss: 0.2715284824371338\n",
      "image 120/500 | loss: 0.2934623658657074\n",
      "image 130/500 | loss: 0.27156904339790344\n",
      "image 140/500 | loss: 0.2673046290874481\n",
      "image 150/500 | loss: 0.29342615604400635\n",
      "image 160/500 | loss: 0.29421550035476685\n",
      "image 170/500 | loss: 0.28537610173225403\n",
      "image 180/500 | loss: 0.2974824905395508\n",
      "image 190/500 | loss: 0.27155908942222595\n",
      "image 200/500 | loss: 0.26588982343673706\n",
      "image 210/500 | loss: 0.2849319279193878\n",
      "image 220/500 | loss: 0.26237040758132935\n",
      "image 230/500 | loss: 0.2779766917228699\n",
      "image 240/500 | loss: 0.2800861895084381\n",
      "image 250/500 | loss: 0.2627136707305908\n",
      "image 260/500 | loss: 0.2612490952014923\n",
      "image 270/500 | loss: 0.2550579309463501\n",
      "image 280/500 | loss: 0.24742978811264038\n",
      "image 290/500 | loss: 0.2842959761619568\n",
      "image 300/500 | loss: 0.24739527702331543\n",
      "image 310/500 | loss: 0.25228482484817505\n",
      "image 320/500 | loss: 0.24498745799064636\n",
      "image 330/500 | loss: 0.24643151462078094\n",
      "image 340/500 | loss: 0.2618699073791504\n",
      "image 350/500 | loss: 0.24379318952560425\n",
      "image 360/500 | loss: 0.24813157320022583\n",
      "image 370/500 | loss: 0.2534872889518738\n",
      "image 380/500 | loss: 0.25083425641059875\n",
      "image 390/500 | loss: 0.2428322285413742\n",
      "image 400/500 | loss: 0.3305494785308838\n",
      "image 410/500 | loss: 0.3476351201534271\n",
      "image 420/500 | loss: 0.3261515498161316\n",
      "image 430/500 | loss: 0.3122774362564087\n",
      "image 440/500 | loss: 0.30916595458984375\n",
      "image 450/500 | loss: 0.3056878447532654\n",
      "image 460/500 | loss: 0.30618566274642944\n",
      "image 470/500 | loss: 0.3632030189037323\n",
      "image 480/500 | loss: 0.32025831937789917\n",
      "image 490/500 | loss: 0.3518672585487366\n",
      "Scene 0/1, Frame 2/3 complete\n",
      "image 0/500 | loss: 0.22194412350654602\n",
      "image 10/500 | loss: 0.17703500390052795\n",
      "image 20/500 | loss: 0.17437469959259033\n",
      "image 30/500 | loss: 0.17235267162322998\n",
      "image 40/500 | loss: 0.17006203532218933\n",
      "image 50/500 | loss: 0.20624524354934692\n",
      "image 60/500 | loss: 0.16878393292427063\n",
      "image 70/500 | loss: 0.16822001338005066\n",
      "image 80/500 | loss: 0.16853293776512146\n",
      "image 90/500 | loss: 0.17103475332260132\n",
      "image 100/500 | loss: 0.16794897615909576\n",
      "image 110/500 | loss: 0.16763153672218323\n",
      "image 120/500 | loss: 0.16777664422988892\n",
      "image 130/500 | loss: 0.16749207675457\n",
      "image 140/500 | loss: 0.16837868094444275\n",
      "image 150/500 | loss: 0.17125394940376282\n",
      "image 160/500 | loss: 0.16919800639152527\n",
      "image 170/500 | loss: 0.16812872886657715\n",
      "image 180/500 | loss: 0.1697923094034195\n",
      "image 190/500 | loss: 0.1697554886341095\n",
      "image 200/500 | loss: 0.16773487627506256\n",
      "image 210/500 | loss: 0.16858810186386108\n",
      "image 220/500 | loss: 0.17388035356998444\n",
      "image 230/500 | loss: 0.1690494418144226\n",
      "image 240/500 | loss: 0.16877272725105286\n",
      "image 250/500 | loss: 0.16864724457263947\n",
      "image 260/500 | loss: 0.1693933755159378\n",
      "image 270/500 | loss: 0.17021121084690094\n",
      "image 280/500 | loss: 0.1709069013595581\n",
      "image 290/500 | loss: 0.1880306601524353\n",
      "image 300/500 | loss: 0.16835647821426392\n",
      "image 310/500 | loss: 0.1813502013683319\n",
      "image 320/500 | loss: 0.16732460260391235\n",
      "image 330/500 | loss: 0.16768862307071686\n",
      "image 340/500 | loss: 0.16759373247623444\n",
      "image 350/500 | loss: 0.16826020181179047\n",
      "image 360/500 | loss: 0.16875828802585602\n",
      "image 370/500 | loss: 0.16656488180160522\n",
      "image 380/500 | loss: 0.16611991822719574\n",
      "image 390/500 | loss: 0.16642378270626068\n",
      "image 400/500 | loss: 0.16653656959533691\n",
      "image 410/500 | loss: 0.17504623532295227\n",
      "image 420/500 | loss: 0.16822603344917297\n",
      "image 430/500 | loss: 0.16678845882415771\n",
      "image 440/500 | loss: 0.16820886731147766\n",
      "image 450/500 | loss: 0.1672171950340271\n",
      "image 460/500 | loss: 0.16920031607151031\n",
      "image 470/500 | loss: 0.1675793081521988\n",
      "image 480/500 | loss: 0.16766223311424255\n",
      "image 490/500 | loss: 0.1699059009552002\n",
      "Scene 0/1, Frame 3/3 complete\n"
     ]
    }
   ],
   "source": [
    "steps = 500\n",
    "num_interp = 25\n",
    "\n",
    "for i, (scene, model_identifier) in enumerate(all_subdir_tensors_and_models):\n",
    "    # Decide which model to use based on the model_identifier\n",
    "    if model_identifier == 'MAIN':\n",
    "        g_model = g_model_1\n",
    "    else:\n",
    "        g_model = g_model_2 \n",
    "\n",
    "    this_scene_ws_emas = []\n",
    "    for j, frame_tensor in enumerate(scene):\n",
    "        ws_ema = run_projector(projection_target=frame_tensor,\n",
    "                               g_model=g_model, \n",
    "                               steps=steps,\n",
    "                               perceptual_model=vgg16, \n",
    "                               device=device, \n",
    "                               save_path=None)\n",
    "        this_scene_ws_emas.append(ws_ema)\n",
    "        print(f'Scene {i}/{len(all_subdir_tensors_and_models)}, Frame {j+1}/{len(scene)} complete')\n",
    "\n",
    "    interp_vals = np.linspace(1./num_interp, 1, num=num_interp)\n",
    "    this_scene_latent_interps = []\n",
    "\n",
    "    for j in range(len(this_scene_ws_emas) - 1):\n",
    "        latent_a_np = this_scene_ws_emas[j].cpu().detach().numpy().squeeze()\n",
    "        latent_b_np = this_scene_ws_emas[j+1].cpu().detach().numpy().squeeze()\n",
    "        latent_interp = np.array([slerp(v, latent_a_np, latent_b_np) for v in interp_vals], dtype=np.float32)\n",
    "        this_scene_latent_interps.append(latent_interp)\n",
    "\n",
    "    image_folder_name = f\"generated_frames/{model_identifier}/scene_038\"\n",
    "    if not os.path.exists(image_folder_name):\n",
    "        os.makedirs(image_folder_name)\n",
    "\n",
    "    start_index = 0\n",
    "\n",
    "    for k, latent_interp in enumerate(this_scene_latent_interps):\n",
    "        for j, step in enumerate(latent_interp):\n",
    "            step = torch.tensor(step).unsqueeze(0).to(device)\n",
    "            image_tensor = g_model.synthesis(step, noise_mode='const')\n",
    "            image = transforms.functional.to_pil_image(image_tensor.clamp(-1, 1).add(1).div(2).cpu().squeeze(0))\n",
    "            # Calculate the image name index based on start_index and the current loop iteration\n",
    "            image_name_index = start_index + k * len(latent_interp) + j\n",
    "            # Save the image with the calculated name index\n",
    "            image.save(f'./{image_folder_name}/{image_name_index:04}.jpg')            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
