# Who the GAN is Conor?
_"How do you know that your memories are real?"_
![A grid of fake images generated by the model](./images/fakes_main.png)

### [▶️ View Side-by-Side Comparison](https://vimeo.com/962221187/9dc567cfb0?share=copy) | [▶️ View Final Generated Video](https://vimeo.com/962159643/ce36485b86?share=copy)

This project uses StyleGAN3 model, fine-tuned on frames from of _Who The Fuck Is Conor?_, to reconstruct the same film by projecting the training data into the model's latent space. 

## About
This project was developed as part of my MSc in Creative Computing, and influenced by Terence Broad's [_Blade Runner—Autoencoded_](https://terencebroad.com/works/blade-runner-autoencoded). The goal was to create a complete, somewhat watchable algorithmic film. 

The source material, _Who the Fuck is Conor?_ is a film I co-directed with Anna Plowden in December 2021, which aired on a local Amsterdam TV channel on Christmas day. The filmmaking process involved roleplay and re-scripted improvisation. The actors wear fake mics, and at points they're not acting. This film was influenced by _Symbiopsychotaxiplasm_ (William Greaves), and _Heaven Knows What_ (Safdie Brothers), both mixing reality and fiction in uncomfortable ways. 

This made it an interesting candidate for feeding through a GAN, pushing it through another layer of simulated realty.

## Technical Pipeline
The project was broken down into four main engineering stages:

**1. Data Preparation & Pre-processing**
* The original film was cropped to 1024x1024 and separated into two datasets (main scenes and titles/credits) to be learned by different models.
* Footage was tracked in Premiere Pro to maintain focus on the actors' faces.
* Used Premiere Pro's cut detection to segment the film into shots, then wrote a custom Python script (`extract_frames.py`) to extract thousands of frames into a structured directory for training. It's segmented into shots so as to retain hard cuts from the original.

**2. Model Training**
* Fine-tuned two StyleGAN3 models in PyTorch on the custom datasets.
* The primary model for the main scenes was trained for **380k images**.
* The secondary model for titles was trained for **60k images**.

**3. Frame Projection & Latent Space Interpolation**
* Developed a projection pipeline, based on a class repository, to map keyframes from the original film into the GAN's latent space.
* To make the project computationally (and temporally) feasible, instead of projecting all frames, I only project keyframes (1 per second) and then interpolate the path between them in latent space to generate smooth video transitions. This results in the classic GAN swirling effect.

**4. Video Reconstruction**
* Used a custom OpenCV script (`vid_from_frames.py`) to stitch the sequences of generated frames back into individual video shots.
* Performed final assembly and audio synchronization in Premiere Pro.

## Tech Stack
* **Primary:** Python, PyTorch, StyleGAN3, OpenCV
* **Supporting:** Jupyter Notebooks, Premiere Pro (for data prep), GitHub Copilot (for script boilerplate)

## Challenges and learning outcomes
* **Performance Optimization:** The key technical challenge was the computational cost of frame projection. The solution of projecting keyframes and interpolating in latent space was a critical optimization that made the project possible, reducing the projection workload by a factor of ~25.
* **Synchronization & Debugging:** An initial version of the interpolation logic assumed that each shot ended on a multiple of 25 frames. This caused a loss of 30 seconds of runtime. This was identified and solved by updating the frame extraction script to explicitly include the final frame of each clip as a definite endpoint for interpolation, but it was too late to regenerate the entire sequence so there are some hard cuts within sequences at points. 
* **Future Work:** A key area for future development is to create a fully automated pipeline that uses metadata from the edit detection phase to programmatically sequence the final rendered shots, removing the need for manual assembly in Premiere Pro.

## Acknowledgements
* This project's methodology was heavily inspired by a StyleGAN inference notebook from Rebecca Fiebrink's 'Exploring Machine Intelligence' course at UAL.
* The original film, *Who The Fuck Is Conor?*, was co-created with Anna Plowden.
